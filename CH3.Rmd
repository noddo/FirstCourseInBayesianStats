---
title: "CH3"
author: "Nick Oddo"
date: "2024-08-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(directlabels)
library(reshape2)

```

## 3.1 
Sample survey: Suppose we are going to sample 100 individuals from
a county (of size much larger than 100) and ask each sampled person
whether they support policy $Z$ or not. Let $Y_i = 1$ if person $i$ in the sample
supports the policy, and $Y_i = 0$ otherwise.

a. Assume $Y_1,\ldots, Y_{100}$ are, conditional on $\theta$, i.i.d. binary random variables with expectation $\theta$. 
Write down the joint distribution of $Pr(Y_1 = y_1, . . . , Y_{100} = y_{100}|\theta)$ in a compact form.
Also write down the form of $P(\sum Y_i = y|\theta)$.

The joint distribution (likelihood) of $Pr(Y_1 = y_1, . . . , Y_{100} = y_{100}|\theta)$ is:
\begin{align}
 &Pr(Y_1 = y_1, . . . , Y_{100} = y_{100}|\theta) = \prod p^{y_i} \cdot (1-p)^{(1-y_i)}\\
 &=  p^{\sum_{i=1}^{100} y_i} \cdot (1-p)^{(100-\sum_{i=1}^{100} y_i)}

\end{align}


as for $P(\sum Y_i = y|\theta)$, let $Z = \sum Y_i $. Then:
\begin{align}

&P(\sum Y_i = y|\theta) = P(Z = z|\theta)\\
&= \binom{100}{z} p^{z}(1-p)^{100-z}

\end{align}


b. For the moment, suppose you believed that $\theta \in {0.0, 0.1,\ldots , 0.9, 1.0}$.
Given that the results of the survey were $\sum_{i=1}^{100} = 57$ compute $P(\sum Yi = 57|\theta)$ for each of these 11 values of $\theta$ and plot these probabilities as a function of $\theta$.

```{r}

theta = seq(from = 0, to = 1, length.out = 11)
probs = dbinom(x = 57, size = 100,prob = theta)

data.frame(theta,"probs" = round(probs,4))

plot(theta,probs, type = "b")

```


c. Now suppose you originally had no prior information to believe one of
these $\theta$-values over another, and so $Pr(\theta = 0.0) = Pr(\theta = 0.1) = · · · =Pr(\theta = 0.9) = Pr(\theta = 1.0)$.
Use Bayes’ rule to compute $p(\theta|\sum_i=1 Yi =57)$ for each $\theta$-value. Make a plot of this posterior distribution as a
function of $\theta$.

Using a discrete uniform prior $p(\theta) = \frac{1}{11}$, and using $Y = \sum_{i=1}^{100} Y_i$:

\begin{align}
&P(\theta | Y = 57) \\
&= \frac{ P(Y = 57 | \theta)P(\theta)}{\sum_{j=1}^{11} P(Y = 57 |\theta_j)P(\theta_j) }\\
&=\frac{\binom{100}{57} \theta^{57}(1-\theta)^{43} \cdot \frac{1}{11} }{ \sum_{j=1}^{11} \binom{100}{57}(\frac{1}{11})^{57}(\frac{10}{11})^{43} \cdot (\frac{1}{11})}\\
\end{align}

```{r}

thetas.c = seq(from = 0, to = 1, length.out = 11)
my_post<-function(theta){ (dbinom(57,100,theta)*(1/11))/(sum(dbinom(57,100,theta))*(1/11)) }

data.frame(thetas.c,"probs" = round(my_post(thetas.c),4))

plot(thetas.c,my_post(thetas.c),type = 'b')

```


d. Now suppose you allow $\theta$ to be any value in the interval $[0, 1]$. Using
the uniform prior density for $\theta$, so that $p(\theta) = 1$, plot the posterior
density $p(θ) P(\sum_{i=1}^{100} Y_i = 57|θ)$ as a function of $\theta$.


```{r}

thetas.d = seq(from = 0, to = 1, length.out = 1000)
my_post<-function(theta){ (dbinom(57,100,theta)*(1/11))/(sum(dbinom(57,100,theta))*(1/11)) }

# data.frame(thetas.d,"probs" = my_post(thetas.d))

plot(thetas.d,my_post(thetas.d),type = 'l',xlab = 'theta', ylab = 'posterior prob')

```

e. As discussed in this chapter, the posterior distribution of θ is beta(1+
57, 1 + 100−57). Plot the posterior density as a function of θ. Discuss
the relationships among all of the plots you have made for this exercise.

Assuming a flat prior ($\theta \sim Beta(1,1)$) where $f(\theta) = 1 $ the posterior becomes $\theta|\mathcal{D} \sim Beta(y+1,100-y+1)$. So, 

```{r}

thetas.e = seq(0,1,length.out = 900)

post.probs.e = dbeta(thetas.e,57+1,100-57+1)

plot(thetas.e,post.probs.e,type = 'l')

```

# 3.2 
Sensitivity analysis: It is sometimes useful to express the parameters $a$ and $b$ in a beta distribution in terms of $\theta_0 = \frac{a}{a+b}$ and $n_0 = a+b$, so that $a = \theta_0n_0$ and $b = (1 − \theta_0)n_0$. Reconsidering the sample survey data in Exercise 3.1, for each combination of $\theta_0 \in \{0.1, 0.2, \ldots, 0.9\}$ and $n_0 \in \{1, 2, 8, 16, 32\}$ find the corresponding $a$, $b$ values and compute $P(\theta >0.5| \sum_{i=1}^{100} Y_i = 57)$ using a beta(a, b) prior distribution for $\theta$. Display the results with a contour plot, and discuss how the plot could be used to explain to someone whether or not they should believe that $\theta > 0.5$, based on the data that $\sum_{i=1}^{100} Y_i = 57$.


```{r}

theta0s = seq(from = .1, to = .9, by = .1)
n0s = c(1,2,8,16,32)

#function to sample from posterior distribution and return P(theta >.5 |Y=57)
f <- function(theta0,n0,num.samples=1000){
  
  s =  rbeta(num.samples, 57+(theta0*n0), 43+(1-theta0)*n0)
  
  return(mean(s>.5))
}


params <- outer(theta0s,n0s, FUN =Vectorize(f) )
rownames(params) <-theta0s
colnames(params) <-n0s 

#form into array using reshape2::melt
params.3.2<-melt(params,varnames=c('theta0','n0'),value.name = 'probs')

params.3.2$a = params.3.2$theta0*params.3.2$n0 
params.3.2$b = (1-params.3.2$theta0)*params.3.2$n0 


ggplot( params.3.2, aes(x = theta0, y = n0, z = probs )) +
  geom_contour()+
  scale_y_continuous(breaks = n0s)+
  scale_x_continuous(breaks = theta0s)+
  geom_dl(aes(label = ..level..), method = "top.pieces", stat = "contour")


```

The output of the plot suggests that for all $n_0$, for $\theta_0 > .5$ the posterior probability that $\theta|Y=57$ is very high (90% or higher). 


# 3.3

Tumor counts: A cancer laboratory is estimating the rate of tumorigenesis
in two strains of mice, A and B. They have tumor count data for 10 mice
in strain A and 13 mice in strain B. Type A mice have been well studied,
and information from other laboratories suggests that type A mice have
tumor counts that are approximately Poisson-distributed with a mean of
12. Tumor count rates for type B mice are unknown, but type B mice are
related to type A mice. The observed tumor counts for the two populations
are

\begin{align}
& y_A = (12, 9, 12, 14, 13, 13, 15, 8, 15, 6);\\
& y_B = (11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7)
\end{align}

```{r}

#store data in arrays yA and yB
yA = c(12, 9, 12, 14, 13, 13, 15, 8, 15, 6)
yB = c(11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7)

```


a) Find the posterior distributions, means, variances and 95% quantilebased confidence intervals for $θ_A$ and $θ_B$, assuming a Poisson sampling
distribution for each group and the following prior distribution:
$θ_A \sim \text{gamma}(120,10)$, $θ_B \sim \text{gamma}(12,1)$, $p(θ_A, θ_B) = p(θ_A) × p(θ_B)$.

Assuming independence between A group and B group, $\theta_A|\mathcal{D}$ and $\theta_B|\mathcal{D}$ can be computed separately. Since $\mathcal{D}|\theta \sim Pois(\theta)$ then then using $\theta \sim \text{gamma}(a,b)$ gives a conjugacy; $\theta_A|\mathcal{D} \sim \text{gamma}(n\bar{x}+a,n+b)$

```{r}

num.samples = 1000

post.theta<-function(x,a,b,num.samples = 1000, xbar = NULL, n=NULL){
  if(is.null(xbar)){
  xbar = mean(x)
  }
  if(is.null(n)){
  n = length(x)
  }
  
  s <- rgamma(num.samples, shape = n*xbar+a, rate = n+b)
  
  return(s)
  
}

post.theta.A.samples <- post.theta(yA,120,10)
post.theta.B.samples <- post.theta(yB,12,1)

df = data.frame( "sample" = cbind(c(post.theta.A.samples,post.theta.B.samples)),
                 "group" = as.factor(rep( c("A","B"), times = c(num.samples,num.samples) ))
)

#display data frame with means, variances and conf intervals for theta
data.frame( "group" = c("theta A","theta B"),
            "mean" = round(as.numeric(lapply(list(post.theta.A.samples ,post.theta.B.samples), FUN = mean )),3), 
            "variance" = round(as.numeric( lapply(list(post.theta.A.samples ,post.theta.B.samples), FUN = var )),3),
            "conf.int" = as.character( lapply(list(post.theta.A.samples ,post.theta.B.samples), FUN = function(x){round(unname(quantile(x, c(.025,.975))),3)} ))
            )

#display densities of each group's theta
ggplot(df, aes(x = sample, group =  group,fill = group))+ geom_density(alpha = .7)

```


b) Compute and plot the posterior expectation of $θ_B$ under the prior distribution $θB ∼ gamma(12×n_0, n_0)$ for each value of $n_0 ∈ {1, 2, . . . , 50}$.
Describe what sort of prior beliefs about $θ_B$ would be necessary in order for the posterior expectation of $θ_B$ to be close to that of $θ_A$.

```{r}

xbarB = mean(yB)
nB = length(yB)

x<-seq(from=0, to=15, by = .01)
n0<- seq(from = 1, to = 50, by = 3)

#plot density curves as they relate to the sampled posterior of thetaA (red)
plot(density(post.theta.A.samples), ylim = c(0,1),xlim = c(7,14),main = "", xlab = "", col = "red", lwd = 2)
expectedvalues = c()
for (i in 1:length(n0)){
  a = 12*n0[i]
  b = n0[i]
  
  y = dgamma(x, shape = nB*xbarB+a, rate = nB+b)
  
  expectedvalues = c(expectedvalues, (nB*xbarB+a)/(nB+b))
  
  lines(x, y, type = "l", col =i, lwd = i*.1, main = "",xlab = "")
}

#plot expected values of E[thetaB] with respect to n0
plot(n0,expectedvalues, type = 'b',ylim = c(8,12.5), ylab = "E[thetaB](n0)")
abline(h = 12,col = "blue")

```

The posteriors with varying values of $n_0$ suggests that there would have to be a very strong belief that there is not a difference between $\theta_B$ and $\theta_A$. This would entail a much, much higher value for $n_0$ than those proposed (>50). In fact, for the posterior expectation to come within .5 of $E[\theta_A]$, $n_0$ would have to be 73.

  b1) Consider the following distribution:
  $$ T_{n_0} = \left| \frac{\theta_B(n_0)|\mathbf{y}_B}{\theta_A | \mathbf{y}_A } - 1 \right| $$  
  What is the lowest value for $n_0$ such that $P(T<\epsilon) < .5$, for $\epsilon \in \{.5, .2, .1, .01 \}$?
```{r}
num.samples = 10000

epsilons <-c(.5, .2, .1, .01)

T.stat <- function (n0) {abs( post.theta(yB,12*n0,n0, num.samples = num.samples)/post.theta(yA,120,10, num.samples = num.samples) - 1 )}


T = 1
n0s = c()
ps = c()

n0 = 13
while(mean(T<.01) < .5){
if( length(n0s) > 1000){break}
T = T.stat(n0)
ps = c(ps, mean(T<.1))
n0s = c(n0s,n0)
# print(n0)
n0 = n0 + 1 
}

plot(n0s,ps,type = "l", ylim = c(.3,1.1))
abline(h = 1)

max(n0s)
# plot(density(T))

quantile(T,c(.025,.25,.5,.75,.975))

```

c) Should knowledge about population A tell us anything about population B? Discuss whether or not it makes sense to have $p(θ_A, θ_B) = p(θ_A) × p(θ_B)$.

The problem setup suggests that independence is a strong assumption, given that "*type B mice are related to type A mice*". It would make sense to formulate a model without the assumption of independence. 


# 3.4

Mixtures of beta priors: Estimate the probability $θ$ of teen recidivism
based on a study in which there were $n = 43$ individuals released from
incarceration and $y = 15$ re-offenders within 36 months.

a) Using a $\text{beta}(2,8)$ prior for $θ$, plot $p(θ), p(y|θ)$ and $p(θ|y)$ as functions
of $θ$. Find the posterior mean, mode, and standard deviation of $θ$.
Find a 95% quantile-based confidence interval.

```{r}
n = 43
y = 15

thetas = seq(0,1,by=.01)

df = data.frame( 
  "theta" = thetas
  , "dist" = rep(c("prior","sampling","post"),c(101,101,101))
  , "prob" = cbind(c(
                    dbeta(thetas,2,8), #prior
                    dbinom(15,43,thetas), #sampling
                    dbeta(thetas,2+15,8+28) #posterior
                    )
                   )
                 )
#show plot of distributions.
ggplot(df,aes(x=theta,y=prob,col = dist))+geom_line()

# sample from posterior distribution
post.samples = rbeta(1000,2+15,8+28)

cat(
  "theta | y",
  paste("mean= ", round(mean(post.samples),3)),
  paste("mode= ", round(unique(post.samples)[which.max(unique(post.samples))],3)),
  paste("std.dev= ",round(sd(post.samples),3)),
  paste("quantile=", "[",round(quantile(post.samples,.025),3),",",round(quantile(post.samples,.975),3),"]")
  
,sep = "\n")

```


b) Repeat a), but using a $\text{beta}(8,2)$ prior for $θ$.
```{r}
n = 43
y = 15

thetas = seq(0,1,by=.01)

df = data.frame( 
  "theta" = thetas
  , "dist" = rep(c("prior","sampling","post"),c(101,101,101))
  , "prob" = cbind(c(
                    dbeta(thetas,8,2), #prior
                    dbinom(15,43,thetas), #sampling
                    dbeta(thetas,8+15,2+28) #posterior
                    )
                   )
                 )
#show plot of distributions.
ggplot(df,aes(x=theta,y=prob,col = dist))+geom_line()

# sample from posterior distribution
post.samples = rbeta(1000,8+15,2+28)

cat(
  "theta | y",
  paste("mean= ", round(mean(post.samples),3)),
  paste("mode= ", round(unique(post.samples)[which.max(unique(post.samples))],3)),
  paste("std.dev= ",round(sd(post.samples),3)),
  paste("quantile=", "[",round(quantile(post.samples,.025),3),",",round(quantile(post.samples,.975),3),"]")
  
,sep = "\n")

```
c) Consider the following prior distribution for θ:

\begin{align}
  P(θ) = \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)} [3\theta(1-\theta)^7 + \theta^7(1-\theta)],
\end{align}


which is a 75-25% mixture of a $\text{beta}(2,8)$ and a $\text{beta}(8,2)$ prior distribution. Plot this prior distribution and compare it to the priors in a)
and b). Describe what sort of prior opinion this may represent.

d) For the prior in c):
  
  i. Write out mathematically $p(θ) × p(y|θ)$ and simplify as much as possible.

  ii. The posterior distribution is a mixture of two distributions you know. Identify these distributions.
  
  iii. On a computer, calculate and plot $p(θ) × p(y|θ)$ for a variety of $θ$ values. Also find (approximately) the posterior mode, and discuss its relation to the modes in a) and b).

e) Find a general formula for the weights of the mixture distribution in d) ii, and provide an interpretation for their values.


# 3.5
3.5 Mixtures of conjugate priors: Let p(y|φ) = c(φ)h(y)exp{φt(y)} be an exponential family model and let p1(φ), . . . pK (φ) be K different members of the conjugate class of prior densities given in Section 3.3. A mixture of conjugate priors is given by p ̃(θ) = PKk=1 wkpk(θ), where the wk’s are all greater than zero and P wk = 1 (see also Diaconis and Ylvisaker (1985)).
a) Identify the general form of the posterior distribution of θ, based on n i.i.d. samples from p(y|θ) and the prior distribution given by p ̃.
b) Repeat a) but in the special case that p(y|θ) = dpois(y,θ) and
p1,...,pK aregammadensities.





