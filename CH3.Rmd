---
title: "CH3"
author: "Nick Oddo"
date: "2024-08-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(directlabels)
library(reshape2)

```

## 3.1 
Sample survey: Suppose we are going to sample 100 individuals from
a county (of size much larger than 100) and ask each sampled person
whether they support policy $Z$ or not. Let $Y_i = 1$ if person $i$ in the sample
supports the policy, and $Y_i = 0$ otherwise.

a. Assume $Y_1,\ldots, Y_{100}$ are, conditional on $\theta$, i.i.d. binary random variables with expectation $\theta$. 
Write down the joint distribution of $Pr(Y_1 = y_1, . . . , Y_{100} = y_{100}|\theta)$ in a compact form.
Also write down the form of $P(\sum Y_i = y|\theta)$.

The joint distribution (likelihood) of $Pr(Y_1 = y_1, . . . , Y_{100} = y_{100}|\theta)$ is:
\begin{align}
 &Pr(Y_1 = y_1, . . . , Y_{100} = y_{100}|\theta) = \prod p^{y_i} \cdot (1-p)^{(1-y_i)}\\
 &=  p^{\sum_{i=1}^{100} y_i} \cdot (1-p)^{(100-\sum_{i=1}^{100} y_i)}

\end{align}


as for $P(\sum Y_i = y|\theta)$, let $Z = \sum Y_i $. Then:
\begin{align}

&P(\sum Y_i = y|\theta) = P(Z = z|\theta)\\
&= \binom{100}{z} p^{z}(1-p)^{100-z}

\end{align}


b. For the moment, suppose you believed that $\theta \in {0.0, 0.1,\ldots , 0.9, 1.0}$.
Given that the results of the survey were $\sum_{i=1}^{100} = 57$ compute $P(\sum Yi = 57|\theta)$ for each of these 11 values of $\theta$ and plot these probabilities as a function of $\theta$.

```{r}

theta = seq(from = 0, to = 1, length.out = 11)
probs = dbinom(x = 57, size = 100,prob = theta)

data.frame(theta,"probs" = round(probs,4))

plot(theta,probs, type = "b")

```


c. Now suppose you originally had no prior information to believe one of
these $\theta$-values over another, and so $Pr(\theta = 0.0) = Pr(\theta = 0.1) = · · · =Pr(\theta = 0.9) = Pr(\theta = 1.0)$.
Use Bayes’ rule to compute $p(\theta|\sum_i=1 Yi =57)$ for each $\theta$-value. Make a plot of this posterior distribution as a
function of $\theta$.

Using a discrete uniform prior $p(\theta) = \frac{1}{11}$, and using $Y = \sum_{i=1}^{100} Y_i$:

\begin{align}
&P(\theta | Y = 57) \\
&= \frac{ P(Y = 57 | \theta)P(\theta)}{\sum_{j=1}^{11} P(Y = 57 |\theta_j)P(\theta_j) }\\
&=\frac{\binom{100}{57} \theta^{57}(1-\theta)^{43} \cdot \frac{1}{11} }{ \sum_{j=1}^{11} \binom{100}{57}(\frac{1}{11})^{57}(\frac{10}{11})^{43} \cdot (\frac{1}{11})}\\
\end{align}

```{r}

thetas.c = seq(from = 0, to = 1, length.out = 11)
my_post<-function(theta){ (dbinom(57,100,theta)*(1/11))/(sum(dbinom(57,100,theta))*(1/11)) }

data.frame(thetas.c,"probs" = round(my_post(thetas.c),4))

plot(thetas.c,my_post(thetas.c),type = 'b')

```


d. Now suppose you allow $\theta$ to be any value in the interval $[0, 1]$. Using
the uniform prior density for $\theta$, so that $p(\theta) = 1$, plot the posterior
density $p(θ) P(\sum_{i=1}^{100} Y_i = 57|θ)$ as a function of $\theta$.


```{r}

thetas.d = seq(from = 0, to = 1, length.out = 1000)
my_post<-function(theta){ (dbinom(57,100,theta)*(1/11))/(sum(dbinom(57,100,theta))*(1/11)) }

# data.frame(thetas.d,"probs" = my_post(thetas.d))

plot(thetas.d,my_post(thetas.d),type = 'l',xlab = 'theta', ylab = 'posterior prob')

```

e. As discussed in this chapter, the posterior distribution of θ is beta(1+
57, 1 + 100−57). Plot the posterior density as a function of θ. Discuss
the relationships among all of the plots you have made for this exercise.

Assuming a flat prior ($\theta \sim Beta(1,1)$) where $f(\theta) = 1 $ the posterior becomes $\theta|\mathcal{D} \sim Beta(y+1,100-y+1)$. So, 

```{r}

thetas.e = seq(0,1,length.out = 900)

post.probs.e = dbeta(thetas.e,57+1,100-57+1)

plot(thetas.e,post.probs.e,type = 'l')

```

# 3.2 
Sensitivity analysis: It is sometimes useful to express the parameters $a$ and $b$ in a beta distribution in terms of $\theta_0 = \frac{a}{a+b}$ and $n_0 = a+b$, so that $a = \theta_0n_0$ and $b = (1 − \theta_0)n_0$. Reconsidering the sample survey data in Exercise 3.1, for each combination of $\theta_0 \in \{0.1, 0.2, \ldots, 0.9\}$ and $n_0 \in \{1, 2, 8, 16, 32\}$ find the corresponding $a$, $b$ values and compute $P(\theta >0.5| \sum_{i=1}^{100} Y_i = 57)$ using a beta(a, b) prior distribution for $\theta$. Display the results with a contour plot, and discuss how the plot could be used to explain to someone whether or not they should believe that $\theta > 0.5$, based on the data that $\sum_{i=1}^{100} Y_i = 57$.


```{r}

theta0s = seq(from = .1, to = .9, by = .1)
n0s = c(1,2,8,16,32)

#function to sample from posterior distribution and return P(theta >.5 |Y=57)
f <- function(theta0,n0,num.samples=1000){
  
  s =  rbeta(num.samples, 57+(theta0*n0), 43+(1-theta0)*n0)
  
  return(mean(s>.5))
}


params <- outer(theta0s,n0s, FUN =Vectorize(f) )
rownames(params) <-theta0s
colnames(params) <-n0s 

#form into array using reshape2::melt
params.3.2<-melt(params,varnames=c('theta0','n0'),value.name = 'probs')

params.3.2$a = params.3.2$theta0*params.3.2$n0 
params.3.2$b = (1-params.3.2$theta0)*params.3.2$n0 


ggplot( params.3.2, aes(x = theta0, y = n0, z = probs )) +
  geom_contour()+
  scale_y_continuous(breaks = n0s)+
  scale_x_continuous(breaks = theta0s)+
  geom_dl(aes(label = ..level..), method = "top.pieces", stat = "contour")


```

The output of the plot suggests that for all $n_0$, for $\theta_0 > .5$ the posterior probability that $\theta|Y=57$ is very high (90% or higher). 


# 3.3

Tumor counts: A cancer laboratory is estimating the rate of tumorigenesis
in two strains of mice, A and B. They have tumor count data for 10 mice
in strain A and 13 mice in strain B. Type A mice have been well studied,
and information from other laboratories suggests that type A mice have
tumor counts that are approximately Poisson-distributed with a mean of
12. Tumor count rates for type B mice are unknown, but type B mice are
related to type A mice. The observed tumor counts for the two populations
are

\begin{align}
& y_A = (12, 9, 12, 14, 13, 13, 15, 8, 15, 6);\\
& y_B = (11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7)
\end{align}

```{r}

#store data in arrays yA and yB
yA = c(12, 9, 12, 14, 13, 13, 15, 8, 15, 6)
yB = c(11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7)

```


a) Find the posterior distributions, means, variances and 95% quantilebased confidence intervals for $θ_A$ and $θ_B$, assuming a Poisson sampling
distribution for each group and the following prior distribution:
$θ_A \sim \text{gamma}(120,10)$, $θ_B \sim \text{gamma}(12,1)$, $p(θ_A, θ_B) = p(θ_A) × p(θ_B)$.

Assuming independence between A group and B group, $\theta_A|\mathcal{D}$ and $\theta_B|\mathcal{D}$ can be computed separately. Since $\mathcal{D}|\theta \sim Pois(\theta)$ then then using $\theta \sim \text{gamma}(a,b)$ gives a conjugacy; $\theta_A|\mathcal{D} \sim \text{gamma}(n\bar{x}+a,n+b)$

```{r}

num.samples = 1000

post.theta<-function(x,a,b,num.samples = 1000, xbar = NULL, n=NULL){
  if(is.null(xbar)){
  xbar = mean(x)
  }
  if(is.null(n)){
  n = length(x)
  }
  
  s <- rgamma(num.samples, shape = n*xbar+a, rate = n+b)
  
  return(s)
  
}

post.theta.A.samples <- post.theta(yA,120,10)
post.theta.B.samples <- post.theta(yB,12,1)

df = data.frame( "sample" = cbind(c(post.theta.A.samples,post.theta.B.samples)),
                 "group" = as.factor(rep( c("A","B"), times = c(num.samples,num.samples) ))
)

#display data frame with means, variances and conf intervals for theta
data.frame( "group" = c("theta A","theta B"),
            "mean" = round(as.numeric(lapply(list(post.theta.A.samples ,post.theta.B.samples), FUN = mean )),3), 
            "variance" = round(as.numeric( lapply(list(post.theta.A.samples ,post.theta.B.samples), FUN = var )),3),
            "conf.int" = as.character( lapply(list(post.theta.A.samples ,post.theta.B.samples), FUN = function(x){round(unname(quantile(x, c(.025,.975))),3)} ))
            )

#display densities of each group's theta
ggplot(df, aes(x = sample, group =  group,fill = group))+ geom_density(alpha = .7)

```


b) Compute and plot the posterior expectation of $θ_B$ under the prior distribution $θB ∼ gamma(12×n_0, n_0)$ for each value of $n_0 ∈ {1, 2, . . . , 50}$.
Describe what sort of prior beliefs about $θ_B$ would be necessary in order for the posterior expectation of $θ_B$ to be close to that of $θ_A$.

```{r}

xbarB = mean(yB)
nB = length(yB)

x<-seq(from=0, to=15, by = .01)
n0<- seq(from = 1, to = 50, by = 3)

#plot density curves as they relate to the posterior of thetaA
plot(density(post.theta.A.samples), ylim = c(0,1),xlim = c(7,14),main = "", xlab = "", col = "red", lwd = 2)
expectedvalues = c()
for (i in 1:length(n0)){
  a = 12*n0[i]
  b = n0[i]
  
  y = dgamma(x, shape = nB*xbarB+a, rate = nB+b)
  
  expectedvalues = c(expectedvalues, (nB*xbarB+a)/(nB+b))
  
  lines(x, y, type = "l", col =i, lwd = i*.1, main = "",xlab = "")
}

# lines(density(post.theta.A.samples), ylim = c(0,1), xlim = c(7,14), lty = 9, xlab = "", main = "thetaB|data ~ gamma(n*xbar+a,n+b) ")


plot(n0,expectedvalues, type = 'b',ylim = c(8,12.5), ylab = "E[thetaB](n0)")
abline(h = 12,col = "blue")

```
The posteriors with varying values of $n_0$ suggests that there would have to be a very strong belief that there is not a difference between $\theta_B$ and $\theta_A$. This would entail a much, much higher value for $n_0$ than those proposed (>50). In fact, for the posterior expectation to come within .5 of $E[\theta_A]$, $n_0$ would have to be 73.

  b1) Consider the following distribution:
  $$ T_{n_0} = \left| \frac{\theta_A(n_0)|\mathbf{y}_A}{\theta_B | \mathbf{y}_B } - 1 \right| $$  
  What is the lowest value for $n_0$ such that $P(T<\epsilon) < .5$, for $\epsilon \in \{.5, .2, .1, .01 \}$?


c) Should knowledge about population A tell us anything about population B? Discuss whether or not it makes sense to have $p(θ_A, θ_B) = p(θ_A) × p(θ_B)$.

The problem setup suggests that independence is a strong assumption, given that "*type B mice are related to type A mice*". It would make sense to formulate a model without the assumption of independence. 

